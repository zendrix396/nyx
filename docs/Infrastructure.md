# Architecture Deep Dive

Nyx isn't a single program but a system of interconnected modules working in a continuous loop. This document details the high-level architecture, the core components, and the data flow for task execution.

## High-Level View: The Cognitive Loop

The core of Nyx is a "Cognitive Loop" managed by a central Orchestrator. This loop mimics a human's process of perceiving a situation, thinking about it, and then acting.

[image](image.png)

---

## Core Modules

### 1. Orchestrator (Rust Core)
This is the central nervous system. It's a state machine that directs the flow of information between all other modules.
-   **Responsibilities:**
    -   Listens for user input (hotkey, voice).
    -   Manages the current state: `IDLE`, `LISTENING`, `RECORDING`, `EXECUTING`.
    -   Initiates and manages the Cognitive Loop for a given task.
    -   Manages the short-term "Ephemeral Session" context for conversations.

### 2. Perception Module (The Eyes & Ears)
This module's job is to translate the state of the machine into a format the Cognition Module can understand.
-   **Components:**
    -   **Speech-to-Text:** An open-source, local engine (e.g., `whisper.cpp`) to transcribe voice commands into text.
    -   **Screen Capture Engine:** A highly efficient Rust crate (`scrap`) to capture screenshots.
    -   **Differential Frame Analyzer:** An optimization layer that compares the current frame with the last one and only processes the changed regions. This dramatically reduces the analytical load, preventing the agent from analyzing a static background 100 times a second.
    -   **OCR Engine:** A fast OCR model (e.g., `olmoocr` or a similar library) to extract all visible text from the screen or a specific window.
    -   **UI Element Detector:** A lightweight, local computer vision model (e.g., a fine-tuned YOLOv8 or MobileNet) trained to identify common UI elements like buttons, input fields, scrollbars, and icons. This creates a "UI Map" that gives semantic meaning to pixel coordinates.

### 3. Cognition Module (The Brain)
This module decides *what to do*. It takes the user's intent and the perceived state of the system and creates a step-by-step plan.
-   **Core Engine:** Google's Gemini-flash model via API.
-   **Responsibilities:**
    -   **Intent Parsing:** Understands the user's initial high-level goal from natural language.
    -   **Task Planning:** Breaks the goal down into a sequence of calls to available tools. It receives the user prompt and a list of all available tools from the Knowledge Base, then outputs a JSON object representing the plan.
    -   **Tool Genesis:** If a required tool does not exist, this module is responsible for generating the code for a new MCP tool. It uses the LLM's coding ability to write a script (usually in Python or Go for simplicity) and provides it to the Tooling Module to be saved and hosted.

### 4. Tooling Module (The Hands)
This module *executes* the plan created by the Cognition Module. It is the bridge between the agent's digital mind and the operating system.
-   **Components:**
    -   **MCP Client/Host:** Can make requests to any MCP-compliant tool (whether built-in or dynamically generated) and can also host new tools as local servers.
    -   **Low-Level I/O Controller:** A Rust crate (`enigo`, `rdev`) that can directly simulate mouse movements, clicks, and keyboard presses. This is the fallback for interacting with apps that don't have an API or MCP tool.
    -   **Command Runner:** A secure wrapper around Rust's `std::process::Command` to run shell commands and execute scripts generated by the Cognition Module.

### 5. Knowledge Base (Memory)
This is the agent's long-term memory.
-   **Components:**
    -   **Tool Library:** A persistent, indexed collection of all available MCP tools. Each entry contains the tool's name, a natural language description, and the executable script. This is the "permanent toolbox" that grows over time.
    -   **Macro Storage:** A folder containing all user-recorded macros, stored in a structured format (e.g., JSON) that includes the sequence of actions and, for AI-Assisted macros, the associated screenshots and contextual metadata.
    -   **Reinforcement Learning Data:** A log of task trajectories: `(prompt, plan, actions, outcome, user_feedback)`. This data is collected to be used by the Feedback Loop for future fine-tuning of the agent's planning model.

---

## Task Execution Flow: A Concrete Example

Let's trace the user's request: **"Hey Nyx, find all the PDFs I downloaded this week, summarize each one, and move them into my 'Reading List' folder."**

1.  **Input & Perception:**
    -   The user speaks the command. The **Perception Module**'s STT engine transcribes it to text.
    -   The **Orchestrator** receives the text, starts an `EXECUTING` session, and passes the command to the Cognition Module.

2.  **Cognition & Planning:**
    -   The **Cognition Module** receives the prompt. It also receives a list of available tools from the **Knowledge Base**, which might include `file.search`, `file.get_metadata`, `pdf.extract_text`, `llm.summarize`, and `file.move`.
    -   It sends a request to the Gemini-flash API, which looks something like this:
        > "User wants to find PDFs downloaded this week, summarize them, and move them. Generate a JSON plan using the available tools: `file.search(pattern, path)`, `file.get_metadata(path)`, `pdf.extract_text(path)`, `llm.summarize(text)`, `file.move(source, destination)`."
    -   The LLM returns a structured plan:
        ```json
        [
          { "tool": "file.search", "params": { "pattern": "*.pdf", "path": "~/Downloads" }, "output": "pdf_files" },
          { "tool": "filter_by_date", "params": { "files": "pdf_files", "duration": "1_week" }, "output": "recent_pdfs" },
          { "tool": "loop", "collection": "recent_pdfs", "actions": [
            { "tool": "pdf.extract_text", "params": { "path": "loop_item" }, "output": "pdf_content" },
            { "tool": "llm.summarize", "params": { "text": "pdf_content" }, "output": "summary_text" },
            { "tool": "file.write", "params": { "content": "summary_text", "path": "~/Reading List/{{loop_item}}.txt" } },
            { "tool": "file.move", "params": { "source": "loop_item", "destination": "~/Reading List/" } }
          ]}
        ]
        ```

3.  **Execution:**
    -   The **Orchestrator** receives this plan and begins executing it step-by-step.
    -   It calls the **Tooling Module** to run `file.search`.
    -   It processes the filter logic.
    -   It then loops through the filtered list, calling the **Tooling Module** for each step: extracting text, calling the LLM API again for the summary, writing the summary to a file, and finally moving the original PDF. Each action's status is briefly displayed in the minimalist UI overlay.

4.  **Feedback:**
    -   The **Orchestrator** logs the entire process—the initial prompt, the generated plan, and whether it completed successfully—to the **Knowledge Base**. If the user were to interrupt or undo the action, that would also be logged as a negative signal.

---

## Macro Recording & Playback

### Manual Mode
-   **Recording:** The Orchestrator simply logs every single mouse and keyboard event from the `rdev` listener into a list. It's a "dumb," literal recording.
-   **Playback:** The Tooling Module's Low-Level I/O Controller replays the recorded events exactly as they were saved. Fast but brittle.

### AI-Assisted Mode
-   **Recording:** When the user performs a significant action (like a click), the Orchestrator doesn't just save the coordinate. It triggers the **Perception Module** to take a screenshot and analyze the UI element at that coordinate. It saves a contextual object: `{ "action": "click", "target_description": "Button with text 'Submit'", "target_screenshot": "path/to/img.png" }`.
-   **Playback (Self-Correcting):**
    1.  The Orchestrator reads the next step: "Click the button labeled 'Submit'".
    2.  It asks the **Perception Module**: "Find the coordinates of the 'Submit' button on the current screen."
    3.  The Perception Module uses its OCR and UI Detector to find the button, even if it has moved.
    4.  If found, the **Tooling Module** clicks the *new* coordinates.
    5.  If not found, the **Cognition Module** is invoked with the context: "I need to click this button (shows screenshot), but I can't find it on the current screen. What should I do?" The LLM might suggest a corrective action like "scroll down," which the Orchestrator then executes before trying again.

---

## Project Structure

```
nyx-agent/
├── src/                      # Svelte Frontend
│   ├── components/           # UI Components (Overlay, SettingsPanel)
│   ├── App.svelte            # Main UI component
│   └── main.js               # App entry point
├── src-tauri/                # Rust Backend
│   ├── src/
│   │   ├── orchestrator.rs   # Main state machine and cognitive loop
│   │   ├── modules/
│   │   │   ├── perception.rs # Screen capture, OCR, STT, UI detection
│   │   │   ├── cognition.rs  # LLM interaction, planning, tool genesis
│   │   │   ├── tooling.rs    # MCP client/host, low-level I/O
│   │   │   └── knowledge.rs  # Interface to the knowledge base
│   │   ├── tools/            # Built-in and generated MCP tools
│   │   ├── main.rs           # Rust app entry point
│   │   └── lib.rs
│   ├── tauri.conf.json       # Tauri configuration
│   ├── Cargo.toml            # Rust dependencies
│   └── build.rs
├── docs/                     # Project Documentation
│   ├── About.md
│   ├── Architecture.md
│   └── Roadmap.md
├── .gitignore
└── README.md
```
